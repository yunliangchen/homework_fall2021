from collections import OrderedDict

from cs285.critics.dqn_critic import DQNCritic
from cs285.critics.cql_critic import CQLCritic
from cs285.infrastructure.replay_buffer import ReplayBuffer
from cs285.infrastructure.utils import *
from cs285.policies.argmax_policy import ArgMaxPolicy
from cs285.infrastructure.dqn_utils import MemoryOptimizedReplayBuffer
from cs285.exploration.rnd_model import RNDModel
from .dqn_agent import DQNAgent
import numpy as np


class ExplorationOrExploitationAgent(DQNAgent):
    def __init__(self, env, agent_params):
        super(ExplorationOrExploitationAgent, self).__init__(env, agent_params)
        
        self.replay_buffer = MemoryOptimizedReplayBuffer(100000, 1, float_obs=True)
        self.num_exploration_steps = agent_params['num_exploration_steps']
        self.offline_exploitation = agent_params['offline_exploitation']

        # This agent performs pure exploration for `num_exploration_steps`, which is 10k by default.
        # This is done by the the RND algorithm and use the argmax policy which chooses to perform actions that 
        # maximize the exploration critic value to generate state-action tuples to populate the replay buffer for the algorithm.
        # This exploration critic value is a weighted average of the true environment reward and the RND model prediction error on the next state s'
        # mixed_reward = explore_weight * expl_bonus + exploit_weight * re_n
        # We can also calculate the `expl_bonus` in other ways, including count-based exploration methods (such as pseudo counts and EX2) 
        # or prediction error based approaches (such as exploring states with high Temperal Difference (TD) error) or approaches that maximize marginal state entropy.
        # This reward is then used as the "Q-value" for training the DQN critic
        # And the actor is argmax with respect to it.

        # For part 1, explore_weight = 1 and exploit_weight = 0 and the exploration_critic (DQNCritic) is only learning the exploration bonus
        # The actor is the argmax policy based on the exploration_critic during self.num_exploration_steps
        # When performing evaluation for logging, the true reward will thus likely be the worst since the true reward is not used during training.
        # After self.num_exploration_steps, the critic is switched to the exploitation_critic (CQLCritic) to learn a Q-function on the true reward
        # Now when performing evaluation for logging, the argmax policy reflects how good our Q-values are.
        # For part 1, we don't disable data collection. So for the first 10000 steps, the data is collected based on pure exploration, 
        # and after 10000 steps, the data is collected based on epsilon greedy + argmax on the Q-values.
        # But since learning_starts = 2000, the exploration_critic and exploitation_critic don't start learning until 2000 steps.
        
        # For part 2, after the exploration phase to collect the data, we perform offline RL using conservative Q-learning (CQL).
        # To disable data collection, we simply stop adding frames to the replay buffer if self.offline_exploitation = True and t > self.num_exploration_steps.
        # It augments the Q-function training with a regularizer that minimizes the soft-maximum of the Q-values log (sum_a[exp(Q(s; a))]) 
        # and maximizes the Q-value on the state-action pair seen in the dataset, Q(s; a). 
        # The overall CQL objective is given by the standard TD error objective augmented with the CQL regularizer weighted by alpha:
        # minimize 1/N * sum_{i = 1 to N} [{Q(s_i, a_i) - (r(s_i, a_i) + E_pi[Q(s_i', a')])}^2 + alpha * {log(sum_a[exp(Q(s_i; a))]) - Q(s_i, a_i)}]

        # CQL is a generalization of DQN: It has all the components that a DQN has: a q-network and a target q-network (and we can implement double-q learning)
        # When cql_alpha = 0 => DQN, cql_alpha = 0.1 => CQL
        # All the Q-values in the above loss function is produced by the DQN q-network.

        # For CQL, the training of the Q-function only requres the raw environment reward (not the exploration bonus)
        # However, we will consider utilizing a transformed reward function for training the exploitation critic.

        self.exploitation_critic = CQLCritic(agent_params, self.optimizer_spec)
        self.exploration_critic = DQNCritic(agent_params, self.optimizer_spec)
        
        self.exploration_model = RNDModel(agent_params, self.optimizer_spec)
        self.explore_weight_schedule = agent_params['explore_weight_schedule']
        self.exploit_weight_schedule = agent_params['exploit_weight_schedule']
        
        self.actor = ArgMaxPolicy(self.exploration_critic)
        self.eval_policy = ArgMaxPolicy(self.exploitation_critic)
        self.exploit_rew_shift = agent_params['exploit_rew_shift']
        self.exploit_rew_scale = agent_params['exploit_rew_scale']
        self.eps = agent_params['eps']

        self.density_based_exploration = agent_params['use_customized_exploration']

    def train(self, ob_no, ac_na, re_n, next_ob_no, terminal_n):
        log = {}

        # num_exploration_steps = 10000 by default
        if self.t > self.num_exploration_steps:
            # TODO: After exploration is over, set the actor to optimize the extrinsic critic
            # HINT: Look at method ArgMaxPolicy.set_critic
            # whether it is true argmax or soft max depends on whether `use_boltzmann` is True for the actor
            self.actor.set_critic(self.exploitation_critic)
        
        # learning_starts = 2000 by default
        if (self.t > self.learning_starts
                and self.t % self.learning_freq == 0
                and self.replay_buffer.can_sample(self.batch_size)
        ):

            # Get Reward Weights
            # TODO: Get the current explore reward weight and exploit reward weight
            #       using the schedule's passed in (see __init__)
            # COMMENT: Until part 3, explore_weight = 1, and exploit_weight = 0
            explore_weight = self.explore_weight_schedule.value(self.t)
            exploit_weight = self.exploit_weight_schedule.value(self.t)

            # Run Exploration Model #
            # TODO: Evaluate the exploration model on s' to get the exploration bonus
            # HINT: Normalize the exploration bonus, as RND values vary highly in magnitude
            # The raw exploration bonus = ||f_hat(s') - f(s')|| (norm of a vector)
            raw_expl_bonus = self.exploration_model.forward_np(next_ob_no)
            expl_bonus = normalize(raw_expl_bonus, np.mean(raw_expl_bonus), np.std(raw_expl_bonus))

            if self.density_based_exploration:
                expl_bonus = -np.log(self.get_density(next_ob_no))


            # Reward Calculations #
            # TODO: Calculate mixed rewards, which will be passed into the exploration critic
            # HINT: See doc for definition of mixed_reward
            mixed_reward = explore_weight * expl_bonus + exploit_weight * re_n

            # TODO: Calculate the environment reward
            # HINT: For part 1, env_reward is just 're_n'
            #       After this, env_reward is 're_n' shifted by self.exploit_rew_shift,
            #       and scaled by self.exploit_rew_scale
            env_reward = (re_n + self.exploit_rew_shift) * self.exploit_rew_scale

            # Update Critics And Exploration Model #

            # TODO 1): Update the exploration model (based off s')
            # TODO 2): Update the exploration critic (based off mixed_reward)
            # TODO 3): Update the exploitation critic (based off env_reward)
            expl_model_loss = self.exploration_model.update(next_ob_no) # The RND Network f_hat now sees s', and performs a gradient step
            exploration_critic_loss = self.exploration_critic.update(ob_no, ac_na, next_ob_no, mixed_reward, terminal_n)
            exploitation_critic_loss = self.exploitation_critic.update(ob_no, ac_na, next_ob_no, env_reward, terminal_n)

            # Target Networks #
            if self.num_param_updates % self.target_update_freq == 0:
                # TODO: Update the exploitation and exploration target networks
                self.exploration_critic.update_target_network()
                self.exploitation_critic.update_target_network()

            # Logging #
            log['Exploration Critic Loss'] = exploitation_critic_loss['Training Loss']
            log['Exploitation Critic Loss'] = exploration_critic_loss['Training Loss']
            log['Exploration Model Loss'] = expl_model_loss

            if self.exploitation_critic.cql_alpha >= 0:
                log['Exploitation Data q-values'] = exploitation_critic_loss['Data q-values']
                log['Exploitation OOD q-values'] = exploitation_critic_loss['OOD q-values']
                log['Exploitation CQL Loss'] = exploitation_critic_loss['CQL Loss']

            self.num_param_updates += 1

        self.t += 1
        return log


    def step_env(self):
        """
            Step the env and store the transition
            At the end of this block of code, the simulator should have been
            advanced one step, and the replay buffer should contain one more transition.
            Note that self.last_obs must always point to the new latest observation.
        """
        if (not self.offline_exploitation) or (self.t <= self.num_exploration_steps):
            self.replay_buffer_idx = self.replay_buffer.store_frame(self.last_obs)

        perform_random_action = np.random.random() < self.eps or self.t < self.learning_starts

        if perform_random_action:
            action = self.env.action_space.sample()
        else:
            processed = self.replay_buffer.encode_recent_observation()
            action = self.actor.get_action(processed)

        next_obs, reward, done, info = self.env.step(action)
        self.last_obs = next_obs.copy()

        if (not self.offline_exploitation) or (self.t <= self.num_exploration_steps):
            self.replay_buffer.store_effect(self.replay_buffer_idx, action, reward, done)

        if done:
            self.last_obs = self.env.reset()

    def get_density(self, obs):
        num_states = self.replay_buffer.num_in_buffer - 2
        states = self.replay_buffer.obs[:num_states]
        if num_states <= 0: return 0
        
        H, xedges, yedges = np.histogram2d(states[:,0], states[:,1], range=[[0., 1.], [0., 1.]], density=True)
        # argmax will stop at the first True ("In case of multiple occurrences of the maximum values, the indices 
        # corresponding to the first occurrence are returned.")
        densities = np.zeros(obs.shape[0])
        for i in range(obs.shape[0]):
            observation = obs[i, :]
            idx_x = np.argmax(xedges > observation[0]) - 1
            idx_y = np.argmax(yedges > observation[1]) - 1
            densities[i] = H[idx_x, idx_y]
        return densities
